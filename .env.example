# ============================================================================
# Application Configuration
# ============================================================================

# Environment Mode
NODE_ENV=development

# Server Port
PORT=3001

# ============================================================================
# Logging Configuration
# ============================================================================

# Log Level: error, warn, info, debug (default: info)
LOG_LEVEL=info

# ============================================================================
# LLM Provider Configuration (Powered by Vercel AI SDK)
# ============================================================================

# LLM Provider Selection: openai, anthropic, aws, azure, gcp, ollama
LLM_PROVIDER=ollama

# ============================================================================
# Ollama Configuration (used when LLM_PROVIDER=ollama)
# ============================================================================
OLLAMA_SERVER_URL=http://host.docker.internal:11434
OLLAMA_COORDINATOR_MODEL=qwen2.5:1.5b
OLLAMA_AGENT_MODEL=qwen2.5:1.5b
OLLAMA_TRANSLATION_MODEL=qwen2.5:1.5b

# ============================================================================
# OpenAI Configuration (used when LLM_PROVIDER=openai)
# ============================================================================
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_COORDINATOR_MODEL=gpt-4o-mini
# OPENAI_AGENT_MODEL=gpt-4o-mini

# ============================================================================
# Anthropic Claude Configuration (used when LLM_PROVIDER=anthropic)
# ============================================================================
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_COORDINATOR_MODEL=claude-3-5-sonnet-20241022
# ANTHROPIC_AGENT_MODEL=claude-3-5-sonnet-20241022

# ============================================================================
# AWS Bedrock Configuration (used when LLM_PROVIDER=aws)
# ============================================================================
# AWS_BEARER_TOKEN_BEDROCK=your_aws_bedrock_bearer_token_here
# AWS_REGION=us-east-1
# BEDROCK_COORDINATOR_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0
# BEDROCK_AGENT_MODEL=mistral.mistral-large-2402-v1:0

# ============================================================================
# Microsoft Azure OpenAI Configuration (used when LLM_PROVIDER=azure)
# ============================================================================
# AZURE_API_KEY=your_azure_api_key_here
# AZURE_RESOURCE_NAME=your_azure_resource_name
# AZURE_DEPLOYMENT_ID=your_deployment_id
# AZURE_COORDINATOR_MODEL=your_deployment_id
# AZURE_AGENT_MODEL=your_agent_deployment_id

# ============================================================================
# Google Cloud Vertex AI Configuration (used when LLM_PROVIDER=gcp)
# ============================================================================
# Path to your GCP service account JSON credentials file
# GOOGLE_APPLICATION_CREDENTIALS=./credentials/xxxxxxxx-gcp-xxxxxxxx.json
# GCP Project ID (required - AI SDK needs this explicitly)
# GOOGLE_VERTEX_PROJECT=your_gcp_project_id
# GCP Region (optional - defaults to us-central1)
# GOOGLE_VERTEX_LOCATION=us-central1
# Gemini model to use (e.g., gemini-2.0-flash, gemini-1.5-pro, gemini-1.5-flash)
# GCP_COORDINATOR_MODEL=gemini-1.5-flash
# GCP_AGENT_MODEL=gemini-1.5-pro

# ============================================================================
# Prisma AIRS API Configuration (Optional)
# ============================================================================

PRISMA_AIRS_API_URL=https://service.api.aisecurity.paloaltonetworks.com
PRISMA_AIRS_API_TOKEN=your_api_token_here
PRISMA_AIRS_PROFILE_ID=your_profile_id_here
PRISMA_AIRS_PROFILE_NAME=your_profile_name
